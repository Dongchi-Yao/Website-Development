# -*- coding: utf-8 -*-
"""New model for web-risk quantipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WfumOYVGfozHKOUYdM4Uf9dS6uOihaO7

# Part 1. For probability calculation
"""

def set_seed(seed):
    import random
    import torch
    import torch.nn as nn
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
import pandas as pd
import torch
df = pd.read_csv('/content/drive/MyDrive/Paper 1-PostDoc/new_data.csv')


user_data = [4, 3, 1, 4, 2, 2, 4, 5, 0, 0, 1, 2, 2, 1, 0, 1] # The inputs for the model

feature_cols = df.columns[:-5]
sample_feat  = pd.DataFrame([user_data], columns=feature_cols).astype(str)

all_feat = df.iloc[:, :-5].astype(str)
combined = pd.concat([all_feat, sample_feat], ignore_index=True)
df_hot   = pd.get_dummies(combined)
df_hot["1.5_4"] = False
df_hot = df_hot.reindex(sorted(df_hot.columns), axis=1)
sample_tensor = torch.tensor(df_hot.tail(1).values.astype(int), dtype=torch.float)


file_paths = ['/content/drive/MyDrive/Paper 1-PostDoc/mixture_of_experts_model_definition.py']
for file_path in file_paths:
    with open(file_path) as file:
        exec(file.read())
group_info_2=torch.load('/content/drive/MyDrive/Paper 1-PostDoc/group_info_2.pth')

model_ft=MixtureOfExperts(group_info_2,
                             hidden_dim        = 64,
                             output_dim        = 5,
                             expert_depth      = 1,
                             expert_residual   = True,
                             gating_use_mlp    = False,
                             gating_hidden_dim = 128)

checkpoint = torch.load('/content/drive/MyDrive/Paper 1-PostDoc/best_model_ft.pth', map_location=torch.device('cpu'))
model_ft.load_state_dict(checkpoint['model_state_dict'])
model_ft.eval()

with torch.no_grad():
    logits = model_ft(sample_tensor)
    probs = torch.sigmoid(logits) # The probabilities for the five risk types
threshold=0.375
#print (probs>=threshold) # Whether the risk event would happen according to the threshold

"""# Part 2. For risk contribution"""

test_tensor = sample_tensor

import torch, shap, numpy as np
set_seed(0)

class ProbModel(torch.nn.Module):
    """Wraps the original logits model and converts each logit → probability."""
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model        # outputs shape (N, C) logits
    def forward(self, x):
        logits = self.base_model(x)
        return torch.sigmoid(logits)        # shape (N, C) probabilities

# 1️⃣  Build the wrapped model
prob_model = ProbModel(model_ft).eval()     # keep .eval() to freeze BN / dropout

# 2️⃣  Choose a background (a few hundred diverse points is typical)
X_train=torch.load('/content/drive/MyDrive/Paper 1-PostDoc/X_train_ft.pt')
background = X_train[:200]                  # or use shap.kmeans for clustering

# 3️⃣  Create the GradientExplainer on the sigmoid-probability view
explainer = shap.GradientExplainer(prob_model, background)

# 4️⃣  Explain a sample or batch
shap_values = explainer.shap_values(test_tensor)
# shap_values is a list with length = C (one array per label)

# 5️⃣  Verify SHAP's additive identity in probability space
with torch.no_grad():
    expected_value = prob_model(background).mean(dim=0).numpy()   # shape (C,)
    pred_prob      = prob_model(test_tensor).numpy()      # shape (1, C)

error = np.abs(np.sum(shap_values, axis=1) + expected_value - pred_prob).round(3)

def shap_values_raw(shap_values, group_info):
    n_samples, n_features, n_outputs = shap_values.shape

    # Use a single list comprehension to gather records (omitting sample_idx in final data)
    records = [
        (output_idx, group_name, feat_idx, shap_values[sample_idx, feat_idx, output_idx])
        for sample_idx in range(n_samples)
        for output_idx in range(n_outputs)
        for group_name, feat_indices in group_info.items()
        for feat_idx in feat_indices
    ]
    return pd.DataFrame(records, columns=["output_unit", "group_name", "feature_index", "shap_value"])

# Build the 'big_shap_table'
big_shap_table = shap_values_raw(shap_values, group_info_2)
feature_names = df_hot.columns

sorted_shap_table = (
    big_shap_table
    .sort_values(["output_unit", "group_name", "shap_value"], ascending=[True, True, False])
    .assign(
        feature_name=lambda df: df["feature_index"].apply(lambda i: feature_names[i]),
        original_feature=lambda df: df["feature_name"].str[:-2],
    )
    .loc[:, [
        "output_unit",
        "group_name",
        "feature_index",
        "original_feature",
        "feature_name",
        "shap_value"
    ]]
)

sorted_shap_table


import pandas as pd

# ── 1. Per-unit, per-group, per-feature summary ───────────────────────────────
grouped_by_feature = (
    sorted_shap_table
        .groupby(["output_unit", "group_name", "original_feature"], as_index=False)["shap_value"]
        .sum()                                    # sum SHAP values
        .assign(shap_value=lambda d: d["shap_value"].round(4))   # round once
)
# columns → output_unit, group_name, original_feature, shap_value


# ── 2. Feature-level importance across all output_units in each group ─────────
grouped_by_group_and_feature = (
    grouped_by_feature
        .groupby(["group_name", "original_feature"], as_index=False)["shap_value"]
        .sum()
        .rename(columns={"shap_value": "shap_value"})
        .assign(group_importance=lambda d:
                d.groupby("group_name")["shap_value"].transform("sum"))
        .sort_values("original_feature")           # optional tidy order
        .reset_index(drop=True)
)


"""# Part 3. Risk reduction strategy"""

# ------------------------------------------------------------------
# 1) sort each group by shap_value – but keep the original DataFrame
# ------------------------------------------------------------------
sorted_gbf = (  # <- new name
    grouped_by_group_and_feature                     # original stays intact
        .groupby("group_name", group_keys=False)
        .apply(lambda x: x.sort_values("shap_value", ascending=False))
        .reset_index(drop=True)
)

# -------------------------------------------------------
# 2) build per-rank DataFrames and the list-of-lists output
# -------------------------------------------------------
N = sorted_gbf.groupby("group_name")["shap_value"].size().max()

top_dfs = {
    rank: (
        sorted_gbf
            .groupby("group_name")
            .nth(rank-1)      # 0-based inside nth()
            .reset_index()
    )
    for rank in range(1, N+1)  # 1-based ranks
}

all_feature_lists = [
    top_dfs[rank]["original_feature"].dropna().tolist()
    for rank in sorted(top_dfs)
]


# prompt: i want to take the column names (82 columns) of the df_hot, and fill in test_tensor (82 dimensions, value 0 should be converted to false, 1 should be converted to True) and then form another df table

import pandas as pd
# Extract column names from df_hot
df_hot_cols = df_hot.columns.tolist()

# Convert test_tensor to a numpy array and then to booleans
test_array = test_tensor.squeeze().numpy().astype(bool)

# Create a new DataFrame with the column names from df_hot and the boolean values from test_array
test_sample_df = pd.DataFrame([test_array], columns=df_hot_cols)
test_sample_df

updated_indexes = []          # list-of-lists of winning indices
new_df = test_sample_df.copy()
new_dfs = []                  # snapshot after every round
risk_tracking = []            # track risk changes

# Calculate initial risk
x = torch.tensor(new_df.values.astype(int).squeeze(), dtype=torch.float).unsqueeze(0)
with torch.no_grad():
    initial_pred = torch.sigmoid(model_ft(x))
initial_risk = 0.5 * initial_pred.mean() + 0.5 * ((initial_pred > threshold).sum() / 5)
risk_tracking.append(("Initial", initial_risk.item()))

for feature_list in all_feature_lists:
    updated_index = []
    # ── SCORE EACH FEATURE ──────────────────────────────────────────────────
    for target_feature in feature_list:
        subcat_cols = [c for c in new_df.columns if c[:-2] == target_feature]
        if not subcat_cols:
            raise ValueError(f"No columns match feature '{target_feature}'")
        best_idx   = 0
        best_risk  = float('inf')
        # try every level for this feature
        for idx, subcat_col in enumerate(subcat_cols):
            cand = new_df.copy()
            cand[subcat_cols]        = False
            cand.loc[:, subcat_col]  = True

            x = torch.tensor(cand.values.astype(int).squeeze(), dtype=torch.float).unsqueeze(0)

            model_ft.eval()
            with torch.no_grad():
                pred = torch.sigmoid(model_ft(x))

            risk = 0.5 * pred.mean() + 0.5 * ((pred > threshold).sum() / 5)
            if risk < best_risk:
                best_risk = risk
                best_idx  = idx
        updated_index.append(best_idx)
    # ── APPLY THE WINNING COLUMNS ───────────────────────────────────────────
    for target_feature, idx in zip(feature_list, updated_index):
        subcat_cols = [c for c in new_df.columns if c[:-2] == target_feature]
        new_df[subcat_cols]       = False
        new_df.loc[:, subcat_cols[idx]] = True
    
    # Calculate risk after this round
    x = torch.tensor(new_df.values.astype(int).squeeze(), dtype=torch.float).unsqueeze(0)
    with torch.no_grad():
        round_pred = torch.sigmoid(model_ft(x))
    round_risk = 0.5 * round_pred.mean() + 0.5 * ((round_pred > threshold).sum() / 5)
    risk_tracking.append((f"Round {len(updated_indexes) + 1}", round_risk.item()))
    
    updated_indexes.append(updated_index.copy())
    new_dfs.append(new_df.copy())

# Print risk reduction summary
print("\nRisk Reduction Summary:")
print("-" * 50)
for i in range(len(risk_tracking)):
    if i == 0:
        print(f"{risk_tracking[i][0]}: {risk_tracking[i][1]:.4f}")
    else:
        reduction = risk_tracking[i-1][1] - risk_tracking[i][1]
        reduction_percent = (reduction / risk_tracking[i-1][1]) * 100
        print(f"{risk_tracking[i][0]}: {risk_tracking[i][1]:.4f} (Reduction: {reduction:.4f}, {reduction_percent:.2f}%)")

# prompt: i want to make all_feature_lists and updated_indexes a dict associated together

combined_dict = dict(zip(map(tuple, all_feature_lists), updated_indexes))
combined_dict

"""* In the printed output, you could see there are four items in the dict called "combined_dict", which corresponds to four rounds. For example, in round 1, we would address the risk factors of '1.3', '2.1.1', '3.4', '4.3', and the optimized index should be [0, 3, 1, 0] correspondingly. If the overall risk is still severe, we can continue to round 2."""