# -*- coding: utf-8 -*-
"""Mixture-of-Experts model definition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X2qaU42Ywv-n5EpYmuge0EN0X1-T0HAk
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# ------------ Expert ---------------------------------------------------------
class Expert(nn.Module):
    """
    A fully‑connected expert with variable depth, width, and optional residual.
    The final layer outputs raw logits (no Sigmoid) so you can train with
    nn.BCEWithLogitsLoss or nn.CrossEntropyLoss.
    """
    def __init__(
        self,
        input_dim:   int,
        hidden_dim:  int,
        output_dim:  int,
        depth:       int = 1,     # 1 = baseline, 2 = deeper variant
        residual:    bool = False # only meaningful when depth == 1
    ):
        super().__init__()

        if depth == 1:
            core = nn.Linear(input_dim, hidden_dim)
            self.body = nn.Sequential(core, nn.ReLU())
            self.use_residual = residual and (input_dim == hidden_dim)
        elif depth == 2:
            self.body = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU()
            )
            self.use_residual = False
        else:
            raise ValueError("depth must be 1 or 2")

        self.out = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h = self.body(x)
        if self.use_residual:
            h = h + x                       # simple skip connection
        return self.out(h)                  # logits

# ------------ Gating network -------------------------------------------------
class GatingNetwork(nn.Module):
    """
    Returns a softmax over experts.  Two flavours:
      • Linear  : single Linear layer  (baseline)
      • MLP     : Linear → ReLU → Linear  (richer variant)
    """
    def __init__(
        self,
        input_dim:     int,
        num_experts:   int,
        use_mlp:       bool  = False,
        hidden_dim:    int   = 128        # only used if use_mlp = True
    ):
        super().__init__()

        if use_mlp:
            self.net = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, num_experts)
            )
        else:
            self.net = nn.Linear(input_dim, num_experts)

    def forward(self, x):
        return F.softmax(self.net(x), dim=1)   # [batch, num_experts]

# ------------ Mixture‑of‑Experts --------------------------------------------
class MixtureOfExperts(nn.Module):
    """
    group_info : dict[str, list[int]]
                Maps group‑name → list of column indices belonging to that group.
    """
    def __init__(
        self,
        group_info:          dict,
        hidden_dim:          int,
        output_dim:          int            = 5,
        expert_depth:        int            = 1,
        expert_residual:     bool           = False,
        gating_use_mlp:      bool           = False,
        gating_hidden_dim:   int            = 128
    ):
        super().__init__()
        self.group_info   = group_info
        self.group_names  = sorted(group_info.keys())
        self.num_experts  = len(self.group_names)

        # Build experts
        self.experts = nn.ModuleDict()
        for g in self.group_names:
            in_dim = len(group_info[g])
            self.experts[g] = Expert(
                input_dim   = in_dim,
                hidden_dim  = hidden_dim,
                output_dim  = output_dim,
                depth       = expert_depth,
                residual    = expert_residual
            )

        # Build gating network (sees the *whole* input vector)
        total_input_dim = sum(len(cols) for cols in group_info.values())
        self.gating = GatingNetwork(
            input_dim    = total_input_dim,
            num_experts  = self.num_experts,
            use_mlp      = gating_use_mlp,
            hidden_dim   = gating_hidden_dim
        )

    # ----- forward -----------------------------------------------------------
    def forward(self, x):
        """
        x : Tensor[batch, total_input_dim]
        """
        # collect expert outputs
        expert_outs = []
        for g in self.group_names:
            cols = self.group_info[g]
            expert_outs.append(self.experts[g](x[:, cols]))     # [batch, output_dim]
        expert_outs = torch.stack(expert_outs, dim=1)           # [batch, Nexp, output_dim]

        # gating weights
        w = self.gating(x).unsqueeze(-1)                        # [batch, Nexp, 1]

        # weighted sum over experts
        return (expert_outs * w).sum(dim=1)                     # [batch, output_dim] (logits)